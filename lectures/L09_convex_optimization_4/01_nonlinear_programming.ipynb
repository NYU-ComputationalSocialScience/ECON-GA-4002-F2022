{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convex Optimization: Nonlinear Optimization\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Convex Optimization: Theory\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand various algorithms for solving nonlinear optimization problems:\n",
    "    - Bisection for univariate problems\n",
    "    - Gradient Descent\n",
    "    - Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Univariate Optimization: $\\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "We will begin with the task of minimizing a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "We'll also suppose that the only constraint on the optimization problem a bound constraint on $x$\n",
    "\n",
    "The optimization problem is\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_x \\quad & f(x) \\\\\n",
    "s.t. \\quad & a \\le x \\le b\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a test example, we will solve the following problem:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_x \\quad & (x - 3)^2 + 2 \\\\\n",
    "s.t. \\quad & 0 \\le x \\le 10\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    return (x - 3) ** 2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bisection\n",
    "\n",
    "The first algorithm we will learn that can solve this problem is called the bisection algorithm\n",
    "\n",
    "The bisection algorithm works by iteratively splitting the size of the interval containing the optimal $x$ in half, always choosing the half of the interval where the function value is smallest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $a_i$ and $b_i$ represent upper and lower bound of interval on iteration $i$ (note that $a_0 = a$ and $b_0 = b$ from optimization problem)\n",
    "\n",
    "At each step\n",
    "\n",
    "- compute $m_i = (a_i + b_i) / 2$\n",
    "- Compute $fa = f(a_i)$, $fb = f(b_i)$, and $fm = f(m_i)$\n",
    "- Find new interval:\n",
    "    - If $fa < fb$ then $a_{i+1} = a_i$ and $b_{i+1} = m_i$\n",
    "    - Else ($fa \\ge fb$) set $a_{i+1} = m_i$ and $b_{i+1} = b_i$\n",
    "- Stop iterating when $a_i$ and $m_i$ are sufficiently close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's implement the algorithm below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisect_opt(f, a, b, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Find the minimum of the scalar function $f$ on the interval\n",
    "    from $[a, b]$ to a given tolerance\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out: Dict\n",
    "        A dictionary containing keys `x` for the optimizing `x`,\n",
    "        `val` for the optimal function value and `history` containing\n",
    "        a history of iteration progress\n",
    "    \"\"\"\n",
    "    # set up i and hist for tracking algorithm progress\n",
    "    i = 0\n",
    "    hist = []\n",
    "\n",
    "    # compute midpoint and evaluate function at a, b, m\n",
    "    m = (a + b) / 2\n",
    "    fa = f(a)\n",
    "    fb = f(b)\n",
    "    fm = f(m)\n",
    "\n",
    "    while abs(a - m) > tol:\n",
    "        hist.append(dict(\n",
    "            i=i, a=a, b=b, m=m, fa=fa, fb=fb, fm=fm\n",
    "        ))\n",
    "        # choose (a, b) to be smallest two function values\n",
    "        # from (a, m, b)\n",
    "        if fa < fb:\n",
    "            b = m\n",
    "            fb = fm\n",
    "        else:\n",
    "            a = m\n",
    "            fa = fm\n",
    "\n",
    "        # compute new midpoint and increment iteration counter\n",
    "        m = (a + b) / 2\n",
    "        fm = f(m)\n",
    "        i += 1\n",
    "\n",
    "    return dict(x=m, val=fm, history=hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now test our function out on the sample problem (note that we know the optimal answer is x = 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bisect_opt(func1, 0, 10, tol=1e-2)\n",
    "out[\"x\"], len(out[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below we will plot the evolution of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.linspace(0, 10, 50)\n",
    "xstar = 3\n",
    "fx = func1(x)\n",
    "\n",
    "fig, axs = plt.subplots(1, len(out[\"history\"]), figsize=(12, 4))\n",
    "\n",
    "for step, ax in zip(out[\"history\"], axs):\n",
    "    ax.plot(x, fx)\n",
    "    ax.scatter(\n",
    "        [step[\"a\"], step[\"m\"], step[\"b\"]],\n",
    "        [step[\"fa\"], step[\"fm\"], step[\"fb\"]],\n",
    "        label=\"step {}\".format(step[\"i\"]),\n",
    "        c=[\"k\", \"r\", \"c\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(out[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization via derivatives\n",
    "\n",
    "Above we solved the problem of minimizing $f(x)$ by evaluating $f$ at many locations and making intelligent decisions about where to evaluate next\n",
    "\n",
    "We know from calculus that we can optimize $f(x)$ by finding the point $x'$ where $\\frac{df}{dx} = 0$\n",
    "\n",
    "Optimization and derivatives are very closely linked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Recall our example function $f(x) = (x - 3)^2 + 2$\n",
    "\n",
    "For this function we can readily compute the derivative analytically $\\frac{df}{dx} = 2 (x - 3)$\n",
    "\n",
    "Setting the derivative equal to zero and solving reveals that the optimal $x = 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Derivatives With Code\n",
    "\n",
    "For many problems the derivative may either be difficult or impossible to compute analytically\n",
    "\n",
    "In these cases we can rely on the computer to compute or approximate derivatives for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Finite Difference Methods\n",
    "\n",
    "Historically, the most common way to approximate derivatives numerically was to use some form of finite differences\n",
    "\n",
    "These methods are instructive and very helpful when considering applications like solving differential equations\n",
    "\n",
    "A finite difference (FD) method approximates a derivative numerically\n",
    "\n",
    "As an approximation, there will be some error associated with FD approximations\n",
    "\n",
    "If we care only about being able to evaluate the derivative of an arbitrary function at an arbitrary point there is a better way: automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will study automatic differentiation in this lecture\n",
    "\n",
    "There will be some exercises with finite methods on the homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "Automatic differentiation is a family of computer science techniques that allow a computer to evaluate *exact* derivatives *numerically*\n",
    "\n",
    "The details behind how this works are beyond the scope of this lecture\n",
    "\n",
    "In our purposes here we will rely on an implementation of automatic differentiation in the `jax` python library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's dive in to how `jax` works using our example function from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment if you don't have jax\n",
    "# %pip install --user jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from jax import jit, grad\n",
    "\n",
    "def grad_func1(x):\n",
    "    \"Analytical derivative for comparison with jax' output\"\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "jax_grad_func1 = grad(func1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "jax_grad_func1(10.0), grad_func1(10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.array([jax_grad_func1(i) for i in x]) - grad_func1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice we evaluated `grad_func1` on the entire array `x` at once\n",
    "\n",
    "On the other hand, we evaluated `jax_grad_func1` in a loop comprehension\n",
    "\n",
    "Why? *broadcasting*\n",
    "\n",
    "Numpy applies broadcasting over its arguments -- in this case that means it evaluated our derivative at all elements of `x`\n",
    "\n",
    "The functions returned by `jax.grad` do not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Vectorization via `jax.vmap`\n",
    "\n",
    "Vectorization is a relative of numpy's broadcasting that applies a function for all elements in a container\n",
    "\n",
    "`jax` has a function `jax.vmap` that can conusme a function and return a vectorized version\n",
    "\n",
    "This will allow us to compute the derivative at more than one point at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "vec_jax_grad_func1 = vmap(jax_grad_func1)\n",
    "vec_jax_grad_func1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bisecting $\\frac{df}{dx} = 0$\n",
    "\n",
    "Now that we can take derivatives of Python functions, we can apply the optimization technique of finding $x'$ such that $\\frac{df}{dx} |_{x=x'} = 0$\n",
    "\n",
    "The core strategy of the bisection algorithm applies to finding zeros (\"root-finding\" or \"solving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let `df` be a Python function that represents $\\frac{df}{dx}$\n",
    "\n",
    "Given `df` and bounds `a` and `b`, an optimal value for the function `f` can be found as follows\n",
    "\n",
    "- Initialization\n",
    "    - confirm that exactly one of $df(a)$, $df(b)$ is negative\n",
    "    - Set $a_0 = a$ and $b_0 = b$\n",
    "- Iteration\n",
    "    - compute $m_i = (a_i + b_i) / 2$\n",
    "    - Compute $dfa = df(a_i)$, $dfb = df(b_i)$, and $dfm = df(m_i)$\n",
    "    - Find new interval:\n",
    "        - If $dfa \\cdot dfm > 0$ then $a_{i+1} = m_i$ and $b_{i+1} = b_i$\n",
    "        - Else set $a_{i+1} = a_i$ and $b_{i+1} = m_i$\n",
    "    - Stop iterating when $a_i$ and $m_i$ are sufficiently close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise** Implement the `bisect_zero` function below\n",
    "\n",
    "Verify that it works when you pass in `jax_grad_func1` (the correct answer should be `3` as when we call `bisect_opt` above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bisect_zero(df, a, b, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Find the zero of the scalar function $df$ on the interval\n",
    "    from $[a, b]$ to a given tolerance\n",
    "\n",
    "    It must be true that `df(a) * df(b) < 0` so that the interval\n",
    "    [a,b] is said to bracket a zero of `df`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out: Dict\n",
    "        A dictionary containing keys `x` for the optimizing `x`,\n",
    "        `val` for the optimal function value and `history` containing\n",
    "        a history of iteration progress\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comment: Solving vs Optimizing\n",
    "\n",
    "As shown above, the computations required to find the roots or zeros of a function (also called \"solve\" the function) are similar to the computations needed to optimize the function\n",
    "\n",
    "In particular, for any convex function $f$, solving $\\min_x f(x)$ is equivalent to solving $\\frac{df}{dx} = 0$\n",
    "\n",
    "Many of the algorithms used to optimize and solve functions are very similar\n",
    "\n",
    "In the remainder of this lecture we will focus on optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate $x$\n",
    "\n",
    "Now we turn to the case where the domain of $f$ is $n$ dimensional\n",
    "\n",
    "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
    "\n",
    "Define the gradient of $f$ ($\\nabla f$) as $$\\nabla f(x) \\equiv \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\  \\frac{\\partial f}{\\partial x_n}\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many algorithms for solving the multivariate optimization problem\n",
    "\n",
    "Many of them iterate on value of $x$ and follow an update rule of the form $$x_{i+1} = x_i + \\alpha_i p_i$$\n",
    "\n",
    "where $\\alpha_k$ is known as a step size and $p_k$ is the search direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "One algorithm we will study is called *gradient descent*\n",
    "\n",
    "The gradient $\\nabla f(x) |_{x=x_1}$ points in the direction of the greatest *increase* in $f$, at $x_1$\n",
    "\n",
    "It follows that $-\\nabla f(x) |_{x=x_1}$ points in the direction of steepest *decrease* in $f$ at $x_1$\n",
    "\n",
    "If we continue to follow the direction of steepest decrease, we will eventually end up at a point $x^*$ where $\\nabla f(x)|_{x=x^*} = 0$, which means that $x^*$ is optimal value of $f$!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient Descent Algorithm\n",
    "\n",
    "The intuition above can be formalized into an algorithm as follows\n",
    "\n",
    "- Initialization: Choose an initial point $x_0$, a step size $\\alpha$, and a stopping tolerance $\\epsilon$\n",
    "- Iteration: for $i = 0, \\dots$\n",
    "    - Set $x_{i+1} = x_i - \\alpha \\nabla f(x) |_{x_i}$\n",
    "    - If $\\max(\\text{abs}(x_{i+1} - x_i)) < \\epsilon$ return $x_{i+1}$ as $x^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In theory this algorithm *should* work, in practice is often doesn't with a constant step size $\\alpha$\n",
    "\n",
    "A modification to this algorithm would be to compute an optimal step size given the current $x$\n",
    "\n",
    "That is, on iteration $i$ solve the following univariate optimization problem for $\\alpha_i$:\n",
    "\n",
    "$$\\min_{\\alpha_i} f(x_i - \\alpha_i \\nabla f(x) | _{x=x_i})$$\n",
    "\n",
    "We'll implement this algorithm below\n",
    "\n",
    "The sub-problem for finding $\\alpha_i$ is known as line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, x0, tol=1e-5, maxiter=10000):\n",
    "    \"\"\"\n",
    "    Find a local optimum of the function $f$ near x0 using the\n",
    "    gradient descent algorithm\n",
    "    \"\"\"\n",
    "    hist = []\n",
    "    df = grad(f)\n",
    "    i = 0\n",
    "    x = x0\n",
    "    while i < maxiter:\n",
    "        if any(np.isnan(x)): \n",
    "            raise ValueError(\"Got some nans on iteration {}\".format(i))\n",
    "        i += 1\n",
    "        dfx = df(x)\n",
    "        alpha = bisect_opt(lambda a: f(x - a * dfx), 1e-5, 10)[\"x\"]\n",
    "        xp = x - alpha * dfx\n",
    "        diff = max(abs(x - xp))\n",
    "        hist.append(dict(i=i, x=x, dfx=dfx, diff=diff, xp=xp, alpha=alpha))\n",
    "\n",
    "        x = xp\n",
    "        if diff < tol:\n",
    "            break\n",
    "\n",
    "    return dict(x=x, val=f(x), dfx=dfx, history=hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The Rosenbrock function\n",
    "\n",
    "A common function used to test optimization algorithms is called the Rozenbrock function\n",
    "\n",
    "The function can be defined over a domain of an arbitrary number of dimensions, which we'll call $N$\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2) + (1 - x_i)^2$$\n",
    "\n",
    "Let's see what this looks like for $N = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = Axes3D(fig, azim = 135, elev=20)\n",
    "X, Y = np.mgrid[-2:2:0.05, -1:3:0.05]\n",
    "Z = (1 - X)**2 + 100*(Y - X**2)**2\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, norm=LogNorm(), cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's use our `gradient_descent` function to optimize the rosenbrock function in 2d\n",
    "\n",
    "Note that the minimum is `[1, 1]` with a value of 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rosen(x): \n",
    "    return sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "%time rosen_res_gd = gradient_descent(jit(rosen), np.ones(2)*0.5)\n",
    "\n",
    "msg = \"Found optimal value of {} at {} in {} iterations\"\n",
    "print(msg.format(rosen_res_gd[\"val\"], rosen_res_gd[\"x\"], len(rosen_res_gd[\"history\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's Method\n",
    "\n",
    "The method gradient descent method  worked for our problem above, but was rather inefficient\n",
    "\n",
    "It took over 2000 iterations, each of which had an optimization problem to solve\n",
    "\n",
    "This is not uncommon for gradient descent\n",
    "\n",
    "In fact, gradient descent can often lead to oscillating or explosive behavior\n",
    "\n",
    "In either case, we can say that the gradient descent algorithm produced steps that were not improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An alternative to gradient descent that does not have these same symptoms is Newton's method\n",
    "\n",
    "Newton's method is similar to gradient descent, but chooses a step size based on the second derivative of the objective function\n",
    "\n",
    "The optimal step size is found using a second order Taylor series expansion of $f$ around the current $x$ at each iteration (say $x_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This expansion has the following form:\n",
    "\n",
    "$$f(x_i + \\Delta) \\approx f(x_i) + \\Delta^T \\nabla f(x) |_{x=x_i} + \\frac{1}{2}\\Delta^T H(f)(x)|_{x=x_i} \\Delta,$$\n",
    "\n",
    "where $H(f)(x) \\equiv \\begin{bmatrix}\n",
    "  \\dfrac{\\partial^2 f}{\\partial x_1^2} & \\dfrac{\\partial^2 f}{\\partial x_1\\,\\partial x_2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_1\\,\\partial x_n} \\\\[2.2ex]\n",
    "  \\dfrac{\\partial^2 f}{\\partial x_2\\,\\partial x_1} & \\dfrac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_2\\,\\partial x_n} \\\\[2.2ex]\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\[2.2ex]\n",
    "  \\dfrac{\\partial^2 f}{\\partial x_n\\,\\partial x_1} & \\dfrac{\\partial^2 f}{\\partial x_n\\,\\partial x_2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{bmatrix}$ is the hessian matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can optimize the second order TS expansion for $\\Delta$ by taking a derivative, setting equal to zero, and solving for $\\Delta$ (note, I use a shorthand for \"evaluated at $x_i$\")\n",
    "\n",
    "\\begin{align*}\n",
    " 0 &= \\frac{d}{d \\Delta} \\left(f(x_i) + \\Delta^T \\nabla f(x_i)+ \\frac{1}{2}\\Delta^T H(f)(x_i) \\Delta \\right) \\\\\n",
    " 0 &= \\nabla f(x) +  H(f)(x) \\Delta \\\\\n",
    " \\Delta &= -\\left[H(f)(x_i)\\right]^{-1} \\nabla f(x_i)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With this optimal $\\Delta$ in mind, the updating rule for $x$ becomes: $$x_{i+1} = x_i - \\alpha \\left[H(f)(x_i)\\right]^{-1} \\nabla f(x_i),$$\n",
    "\n",
    "where we have added a step size $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use `jax` to construct a Hessian matrix for us as follows (see the [documentation](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#Taking-derivatives-with-grad) for more detail):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def make_hessian(f):\n",
    "    return jit(jacfwd(jacrev(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we are ready to implement Newton's method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(f, x0, alpha=0.1, tol=1e-5, maxiter=1000):\n",
    "    hist = []\n",
    "    df = grad(f)\n",
    "    Hf = make_hessian(f)\n",
    "    i = 0\n",
    "    x = x0\n",
    "    while i < maxiter:\n",
    "        if any(np.isnan(x)): \n",
    "            raise ValueError(\"Got some nans on iteration {}\".format(i))\n",
    "        i += 1\n",
    "        dfx = df(x)\n",
    "        Hfx = Hf(x)\n",
    "        Delta = np.linalg.lstsq(Hfx, dfx, rcond=None)[0]\n",
    "        xp = x - alpha * Delta\n",
    "        diff = max(abs(x - xp))\n",
    "        hist.append(dict(i=i, x=x, dfx=dfx, diff=diff, xp=xp, alpha=alpha, Hfx=Hfx, Delta=Delta))\n",
    "\n",
    "        x = xp\n",
    "        if diff < tol:\n",
    "            break\n",
    "\n",
    "    return dict(x=x, val=f(x), dfx=dfx, history=hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%time rosen_res_newton = newtons_method(rosen, np.ones(2)*0.5)\n",
    "print(msg.format(rosen_res_newton[\"val\"], rosen_res_newton[\"x\"], len(rosen_res_newton[\"history\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! We were able to solve the rosenbrock function in far fewer iterations (~160 instead of ~2100) and far less time\n",
    "\n",
    "This method could be further improved by choosing $\\alpha$ each iteration via line search..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Implement a modified version of Newton's method that incorporates line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method_linesearch(f, x0, tol=1e-5, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Find a local optimium of `f` near `x0` using Newton's method with\n",
    "    a line search for step size\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out: Dict\n",
    "        A dictionary containing keys :\n",
    "        \n",
    "        - `x` for the optimizing `x`\n",
    "        - `val` for the optimal function value  \n",
    "        - `history` containing a history of iteration progress\n",
    "        - `dfx` containing final gradient of f\n",
    "        - `Hfx` containing final Hessian matrix of f\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4841598016aa1c67cc1381635b28aa23a86011301a0baf6a1b6240db5abb8d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
